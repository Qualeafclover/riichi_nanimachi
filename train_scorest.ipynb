{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f1b842d-13ab-4674-b557-19734a7507cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "class MahjongDataset(Dataset):\n",
    "    def __init__(self, data_paths, load=True):\n",
    "        self.x_dict = {\n",
    "            \"x_hand\": None, \n",
    "            \"x_hand_red\": None, \n",
    "            \"x_river\": None, \n",
    "            \"x_river_red\": None, \n",
    "            \"x_river_riichi\": None, \n",
    "            \"x_meld\": None, \n",
    "            \"x_meld_red5\": None, \n",
    "            \"x_meld_throw\": None, \n",
    "            \"x_meld_throw_red\": None, \n",
    "            \"x_dora\": None, \n",
    "            \"x_score\": None, \n",
    "            \"x_pool\": None, \n",
    "            \"x_winds\": None}\n",
    "        self.y = None\n",
    "        self.loads = list(map(np.load, data_paths))\n",
    "        self.loads = [np.load(path) for path in data_paths]\n",
    "\n",
    "        if load:\n",
    "            self.load_index = -1\n",
    "            self.load_next()\n",
    "\n",
    "    def unload(self):\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "        for key in self.x_dict.keys():\n",
    "            self.x_dict[key] = None\n",
    "    \n",
    "    def load_next(self):\n",
    "        self.load_index += 1\n",
    "        self.load_index %= len(self.loads)\n",
    "        self.y = self.loads[self.load_index][\"y\"].astype(np.float32)\n",
    "        for key in self.x_dict.keys():\n",
    "            self.x_dict[key] = self.loads[self.load_index][key].astype(np.float32)\n",
    "        self.post_load()\n",
    "    \n",
    "    def post_load(self):\n",
    "        # Data pre-processing\n",
    "        # Divide by 250 to average out the score values\n",
    "        self.y /= 250\n",
    "        self.x_dict[\"x_score\"] /= 250\n",
    "        self.x_dict[\"x_pool\"] /= 250\n",
    "        # To predict score difference at the end of the round\n",
    "        self.y -= self.x_dict[\"x_score\"]  \n",
    "        # Resize all input to B, X, 24, 9\n",
    "        self.x_dict[\"x_hand\"] = np.repeat(self.x_dict[\"x_hand\"], 6, -2)\n",
    "        self.x_dict[\"x_hand_red\"] = np.expand_dims(self.x_dict[\"x_hand_red\"], (-1, -2))\n",
    "        self.x_dict[\"x_hand_red\"] = np.repeat(self.x_dict[\"x_hand_red\"], 9, -1)\n",
    "        self.x_dict[\"x_hand_red\"] = np.repeat(self.x_dict[\"x_hand_red\"], 24, -2)\n",
    "        self.x_dict[\"x_river\"] = np.reshape(self.x_dict[\"x_river\"], (*self.x_dict[\"x_river\"].shape[:1], -1, *self.x_dict[\"x_river\"].shape[3:]))\n",
    "        self.x_dict[\"x_river_red\"] = np.expand_dims(self.x_dict[\"x_river_red\"], -1)\n",
    "        self.x_dict[\"x_river_red\"] = np.repeat(self.x_dict[\"x_river_red\"], 9, -1)\n",
    "        self.x_dict[\"x_river_riichi\"] = np.expand_dims(self.x_dict[\"x_river_riichi\"], -1)\n",
    "        self.x_dict[\"x_river_riichi\"] = np.repeat(self.x_dict[\"x_river_riichi\"], 9, -1)\n",
    "        self.x_dict[\"x_meld\"] = np.reshape(self.x_dict[\"x_meld\"], (*self.x_dict[\"x_meld\"].shape[:1], -1, *self.x_dict[\"x_meld\"].shape[3:]))\n",
    "        self.x_dict[\"x_meld\"] = np.repeat(self.x_dict[\"x_meld\"], 6, -2)\n",
    "        self.x_dict[\"x_meld_red5\"] = np.reshape(self.x_dict[\"x_meld_red5\"], (self.x_dict[\"x_meld_red5\"].shape[0], -1))\n",
    "        self.x_dict[\"x_meld_red5\"] = np.expand_dims(self.x_dict[\"x_meld_red5\"], (-1, -2))\n",
    "        self.x_dict[\"x_meld_red5\"] = np.repeat(self.x_dict[\"x_meld_red5\"], 9, -1)\n",
    "        self.x_dict[\"x_meld_red5\"] = np.repeat(self.x_dict[\"x_meld_red5\"], 24, -2)\n",
    "        self.x_dict[\"x_meld_throw\"] = np.reshape(self.x_dict[\"x_meld_throw\"], (self.x_dict[\"x_meld_throw\"].shape[0], -1, *self.x_dict[\"x_meld_throw\"].shape[3:]))\n",
    "        self.x_dict[\"x_meld_throw\"] = np.repeat(self.x_dict[\"x_meld_throw\"], 6, -2)\n",
    "        self.x_dict[\"x_meld_throw_red\"] = np.reshape(self.x_dict[\"x_meld_throw_red\"], (self.x_dict[\"x_meld_throw_red\"].shape[0], -1))\n",
    "        self.x_dict[\"x_meld_throw_red\"] = np.expand_dims(self.x_dict[\"x_meld_throw_red\"], (-1, -2))\n",
    "        self.x_dict[\"x_meld_throw_red\"] = np.repeat(self.x_dict[\"x_meld_throw_red\"], 9, -1)\n",
    "        self.x_dict[\"x_meld_throw_red\"] = np.repeat(self.x_dict[\"x_meld_throw_red\"], 24, -2)\n",
    "        self.x_dict[\"x_dora\"] = np.repeat(self.x_dict[\"x_dora\"], 6, -2)\n",
    "        self.x_dict[\"x_score\"] = np.expand_dims(self.x_dict[\"x_score\"], (-1, -2))\n",
    "        self.x_dict[\"x_score\"] = np.repeat(self.x_dict[\"x_score\"], 9, -1)\n",
    "        self.x_dict[\"x_score\"] = np.repeat(self.x_dict[\"x_score\"], 24, -2)\n",
    "        self.x_dict[\"x_pool\"] = np.expand_dims(self.x_dict[\"x_pool\"], (-1, -2, -3))\n",
    "        self.x_dict[\"x_pool\"] = np.repeat(self.x_dict[\"x_pool\"], 9, -1)\n",
    "        self.x_dict[\"x_pool\"] = np.repeat(self.x_dict[\"x_pool\"], 24, -2)\n",
    "        self.x_dict[\"x_winds\"] = np.expand_dims(self.x_dict[\"x_winds\"], -2)\n",
    "        self.x_dict[\"x_winds\"] = np.repeat(self.x_dict[\"x_winds\"], 24, -2)\n",
    "        \n",
    "        # for varname in self.x_dict.keys():\n",
    "        #     print(varname, self.x_dict[varname].shape)\n",
    "\n",
    "        self.x = np.concatenate(list(self.x_dict.values()), axis=1)\n",
    "\n",
    "        # Unload dict to save memory\n",
    "        for key in self.x_dict.keys():\n",
    "            self.x_dict[key] = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# train_ds = MahjongDataset(sorted(glob.glob(\"dataset/riichi_ds_v02/*.npz\"))[:150])\n",
    "# test_ds = MahjongDataset(sorted(glob.glob(\"dataset/riichi_ds_v02/*.npz\"))[150:])\n",
    "\n",
    "full_ds = MahjongDataset(sorted(glob.glob(\"dataset/riichi_ds_v02/*.npz\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a4665f-5ebf-4fbd-9aba-5f01142ecccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=True)\n",
    "# train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "full_dl = torch.utils.data.DataLoader(full_ds, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95960314-82c1-49e1-b645-c336723c843b",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4117ba-f815-43af-b6ad-9a6021baf724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 256, 19, 9]       1,053,952\n",
      "              ReLU-2           [-1, 256, 19, 9]               0\n",
      "       BatchNorm2d-3           [-1, 256, 19, 9]             512\n",
      "            Conv2d-4           [-1, 256, 14, 9]       2,752,768\n",
      "              ReLU-5           [-1, 256, 14, 9]               0\n",
      "       BatchNorm2d-6           [-1, 256, 14, 9]             512\n",
      "            Conv2d-7            [-1, 256, 9, 9]       2,752,768\n",
      "              ReLU-8            [-1, 256, 9, 9]               0\n",
      "       BatchNorm2d-9            [-1, 256, 9, 9]             512\n",
      "           Conv2d-10            [-1, 512, 9, 9]       1,180,160\n",
      "             ReLU-11            [-1, 512, 9, 9]               0\n",
      "      BatchNorm2d-12            [-1, 512, 9, 9]           1,024\n",
      "        MaxPool2d-13            [-1, 512, 3, 3]               0\n",
      "           Conv2d-14           [-1, 1024, 3, 3]       4,719,616\n",
      "             ReLU-15           [-1, 1024, 3, 3]               0\n",
      "      BatchNorm2d-16           [-1, 1024, 3, 3]           2,048\n",
      "          Flatten-17                 [-1, 9216]               0\n",
      "           Linear-18                  [-1, 256]       2,359,552\n",
      "             ReLU-19                  [-1, 256]               0\n",
      "      BatchNorm1d-20                  [-1, 256]             512\n",
      "           Linear-21                    [-1, 4]           1,028\n",
      "================================================================\n",
      "Total params: 14,824,964\n",
      "Trainable params: 14,824,964\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 3.49\n",
      "Params size (MB): 56.55\n",
      "Estimated Total Size (MB): 60.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Increasing layers\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class ModelV1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelV1, self).__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.max_pool = torch.nn.MaxPool2d(3)\n",
    "        \n",
    "        self.conv2d_1 = torch.nn.Conv2d(98, 256, (6, 7), padding=(0, 3))\n",
    "        self.bn_1 = torch.nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv2d_2 = torch.nn.Conv2d(256, 256, (6, 7), padding=(0, 3))\n",
    "        self.bn_2 = torch.nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv2d_3 = torch.nn.Conv2d(256, 256, (6, 7), padding=(0, 3))\n",
    "        self.bn_3 = torch.nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv2d_4 = torch.nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn_4 = torch.nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv2d_5 = torch.nn.Conv2d(512, 1024, 3, padding=1)\n",
    "        self.bn_5 = torch.nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.fc_1 = torch.nn.Linear(9216, 256)\n",
    "        self.bn_6 = torch.nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc_2 = torch.nn.Linear(256, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn_1(self.relu(self.conv2d_1(x)))\n",
    "        x = self.bn_2(self.relu(self.conv2d_2(x)))\n",
    "        x = self.bn_3(self.relu(self.conv2d_3(x)))\n",
    "        x = self.bn_4(self.relu(self.conv2d_4(x)))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.bn_5(self.relu(self.conv2d_5(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn_6(self.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n",
    "summary(ModelV1(), (98, 24, 9), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f702f-8fde-4080-92f7-adc43c110154",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4855a48-a12c-4fb2-a900-bd662bd8bc0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/150:   0%|                                                | 0/872 [00:05<?, ?it/s]\n",
      "Epoch 1/150: 100%|██████████████████████████| 872/872 [01:16<00:00, 11.39it/s, loss=0.041]\n",
      "Epoch 2/150: 100%|██████████████████████████| 832/832 [01:11<00:00, 11.58it/s, loss=0.033]\n",
      "Epoch 3/150: 100%|██████████████████████████| 826/826 [01:12<00:00, 11.35it/s, loss=0.028]\n",
      "Epoch 4/150: 100%|██████████████████████████| 862/862 [01:14<00:00, 11.52it/s, loss=0.029]\n",
      "Epoch 5/150: 100%|██████████████████████████| 833/833 [01:09<00:00, 12.02it/s, loss=0.030]\n",
      "Epoch 6/150: 100%|██████████████████████████| 826/826 [01:10<00:00, 11.77it/s, loss=0.031]\n",
      "Epoch 7/150: 100%|██████████████████████████| 843/843 [01:10<00:00, 11.99it/s, loss=0.030]\n",
      "Epoch 8/150: 100%|██████████████████████████| 852/852 [01:10<00:00, 12.02it/s, loss=0.030]\n",
      "Epoch 9/150: 100%|██████████████████████████| 830/830 [01:09<00:00, 11.99it/s, loss=0.031]\n",
      "Epoch 10/150: 100%|█████████████████████████| 840/840 [01:10<00:00, 11.99it/s, loss=0.029]\n",
      "Epoch 11/150: 100%|█████████████████████████| 814/814 [01:07<00:00, 11.98it/s, loss=0.030]\n",
      "Epoch 12/150: 100%|█████████████████████████| 849/849 [01:10<00:00, 12.00it/s, loss=0.032]\n",
      "Epoch 13/150: 100%|█████████████████████████| 844/844 [01:10<00:00, 12.01it/s, loss=0.029]\n",
      "Epoch 14/150: 100%|█████████████████████████| 831/831 [01:09<00:00, 12.00it/s, loss=0.028]\n",
      "Epoch 15/150: 100%|█████████████████████████| 847/847 [01:10<00:00, 12.01it/s, loss=0.028]\n",
      "Epoch 16/150: 100%|█████████████████████████| 830/830 [01:09<00:00, 11.98it/s, loss=0.029]\n",
      "Epoch 17/150: 100%|█████████████████████████| 833/833 [01:09<00:00, 11.95it/s, loss=0.026]\n",
      "Epoch 18/150: 100%|█████████████████████████| 866/866 [01:12<00:00, 12.01it/s, loss=0.028]\n",
      "Epoch 19/150: 100%|█████████████████████████| 820/820 [01:08<00:00, 12.01it/s, loss=0.027]\n",
      "Epoch 20/150: 100%|█████████████████████████| 834/834 [01:09<00:00, 11.95it/s, loss=0.028]\n",
      "Epoch 21/150: 100%|█████████████████████████| 846/846 [01:10<00:00, 12.00it/s, loss=0.022]\n",
      "Epoch 22/150: 100%|█████████████████████████| 828/828 [01:09<00:00, 12.00it/s, loss=0.023]\n",
      "Epoch 23/150: 100%|█████████████████████████| 802/802 [01:06<00:00, 11.99it/s, loss=0.023]\n",
      "Epoch 24/150: 100%|█████████████████████████| 842/842 [01:10<00:00, 12.01it/s, loss=0.022]\n",
      "Epoch 25/150: 100%|█████████████████████████| 835/835 [01:09<00:00, 12.00it/s, loss=0.021]\n",
      "Epoch 26/150: 100%|█████████████████████████| 850/850 [01:10<00:00, 12.00it/s, loss=0.019]\n",
      "Epoch 27/150: 100%|█████████████████████████| 859/859 [01:11<00:00, 11.99it/s, loss=0.019]\n",
      "Epoch 28/150: 100%|█████████████████████████| 818/818 [01:08<00:00, 11.98it/s, loss=0.018]\n",
      "Epoch 29/150: 100%|█████████████████████████| 845/845 [01:10<00:00, 12.00it/s, loss=0.020]\n",
      "Epoch 30/150: 100%|█████████████████████████| 842/842 [01:10<00:00, 12.00it/s, loss=0.019]\n",
      "Epoch 31/150: 100%|█████████████████████████| 822/822 [01:08<00:00, 12.00it/s, loss=0.017]\n",
      "Epoch 32/150: 100%|█████████████████████████| 817/817 [01:08<00:00, 12.00it/s, loss=0.018]\n",
      "Epoch 33/150: 100%|█████████████████████████| 823/823 [01:08<00:00, 12.01it/s, loss=0.019]\n",
      "Epoch 34/150: 100%|█████████████████████████| 797/797 [01:06<00:00, 11.97it/s, loss=0.019]\n",
      "Epoch 35/150: 100%|█████████████████████████| 827/827 [01:08<00:00, 12.03it/s, loss=0.020]\n",
      "Epoch 36/150: 100%|█████████████████████████| 793/793 [01:06<00:00, 11.97it/s, loss=0.021]\n",
      "Epoch 37/150: 100%|█████████████████████████| 836/836 [01:09<00:00, 11.97it/s, loss=0.021]\n",
      "Epoch 38/150: 100%|█████████████████████████| 850/850 [01:10<00:00, 11.99it/s, loss=0.019]\n",
      "Epoch 39/150: 100%|█████████████████████████| 845/845 [01:10<00:00, 11.98it/s, loss=0.019]\n",
      "Epoch 40/150: 100%|█████████████████████████| 815/815 [01:08<00:00, 11.98it/s, loss=0.018]\n",
      "Epoch 41/150: 100%|█████████████████████████| 823/823 [01:08<00:00, 12.01it/s, loss=0.018]\n",
      "Epoch 42/150: 100%|█████████████████████████| 841/841 [01:10<00:00, 12.01it/s, loss=0.019]\n",
      "Epoch 43/150: 100%|█████████████████████████| 835/835 [01:09<00:00, 12.01it/s, loss=0.020]\n",
      "Epoch 44/150: 100%|█████████████████████████| 817/817 [01:08<00:00, 11.98it/s, loss=0.020]\n",
      "Epoch 45/150: 100%|█████████████████████████| 812/812 [01:07<00:00, 11.97it/s, loss=0.018]\n",
      "Epoch 46/150: 100%|█████████████████████████| 821/821 [01:08<00:00, 11.98it/s, loss=0.018]\n",
      "Epoch 47/150: 100%|█████████████████████████| 828/828 [01:09<00:00, 11.97it/s, loss=0.019]\n",
      "Epoch 48/150: 100%|█████████████████████████| 820/820 [01:08<00:00, 11.96it/s, loss=0.016]\n",
      "Epoch 49/150: 100%|█████████████████████████| 852/852 [01:11<00:00, 11.99it/s, loss=0.017]\n",
      "Epoch 50/150: 100%|█████████████████████████| 827/827 [01:09<00:00, 11.98it/s, loss=0.015]\n",
      "Epoch 51/150: 100%|█████████████████████████| 819/819 [01:08<00:00, 11.99it/s, loss=0.017]\n",
      "Epoch 52/150: 100%|█████████████████████████| 798/798 [01:06<00:00, 11.96it/s, loss=0.017]\n",
      "Epoch 53/150: 100%|█████████████████████████| 814/814 [01:07<00:00, 11.98it/s, loss=0.017]\n",
      "Epoch 54/150: 100%|█████████████████████████| 813/813 [01:08<00:00, 11.94it/s, loss=0.015]\n",
      "Epoch 55/150: 100%|█████████████████████████| 850/850 [01:11<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 56/150: 100%|█████████████████████████| 855/855 [01:11<00:00, 11.96it/s, loss=0.016]\n",
      "Epoch 57/150: 100%|█████████████████████████| 864/864 [01:12<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 58/150: 100%|█████████████████████████| 847/847 [01:10<00:00, 11.98it/s, loss=0.015]\n",
      "Epoch 59/150: 100%|█████████████████████████| 832/832 [01:09<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 60/150: 100%|█████████████████████████| 846/846 [01:10<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 61/150: 100%|█████████████████████████| 840/840 [01:10<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 62/150: 100%|█████████████████████████| 825/825 [01:08<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 63/150: 100%|█████████████████████████| 829/829 [01:09<00:00, 11.96it/s, loss=0.015]\n",
      "Epoch 64/150: 100%|█████████████████████████| 835/835 [01:09<00:00, 11.97it/s, loss=0.018]\n",
      "Epoch 65/150: 100%|█████████████████████████| 840/840 [01:10<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 66/150: 100%|█████████████████████████| 868/868 [01:12<00:00, 11.98it/s, loss=0.017]\n",
      "Epoch 67/150: 100%|█████████████████████████| 830/830 [01:09<00:00, 11.94it/s, loss=0.017]\n",
      "Epoch 68/150: 100%|█████████████████████████| 854/854 [01:11<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 69/150: 100%|█████████████████████████| 818/818 [01:08<00:00, 11.96it/s, loss=0.017]\n",
      "Epoch 70/150: 100%|█████████████████████████| 828/828 [01:09<00:00, 11.97it/s, loss=0.017]\n",
      "Epoch 71/150: 100%|█████████████████████████| 808/808 [01:07<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 72/150: 100%|█████████████████████████| 806/806 [01:07<00:00, 11.96it/s, loss=0.014]\n",
      "Epoch 73/150: 100%|█████████████████████████| 808/808 [01:07<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 74/150: 100%|█████████████████████████| 842/842 [01:10<00:00, 12.00it/s, loss=0.015]\n",
      "Epoch 75/150: 100%|█████████████████████████| 817/817 [01:08<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 76/150: 100%|█████████████████████████| 823/823 [01:08<00:00, 11.95it/s, loss=0.015]\n",
      "Epoch 77/150: 100%|█████████████████████████| 852/852 [01:11<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 78/150: 100%|█████████████████████████| 828/828 [01:09<00:00, 11.95it/s, loss=0.015]\n",
      "Epoch 79/150: 100%|█████████████████████████| 846/846 [01:10<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 80/150: 100%|█████████████████████████| 837/837 [01:10<00:00, 11.96it/s, loss=0.015]\n",
      "Epoch 81/150: 100%|█████████████████████████| 853/853 [01:11<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 82/150: 100%|█████████████████████████| 820/820 [01:08<00:00, 11.96it/s, loss=0.015]\n",
      "Epoch 83/150: 100%|█████████████████████████| 835/835 [01:09<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 84/150: 100%|█████████████████████████| 851/851 [01:11<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 85/150: 100%|█████████████████████████| 832/832 [01:09<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 86/150: 100%|█████████████████████████| 836/836 [01:09<00:00, 11.95it/s, loss=0.015]\n",
      "Epoch 87/150: 100%|█████████████████████████| 856/856 [01:11<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 88/150: 100%|█████████████████████████| 816/816 [01:08<00:00, 11.98it/s, loss=0.014]\n",
      "Epoch 89/150: 100%|█████████████████████████| 812/812 [01:07<00:00, 11.97it/s, loss=0.014]\n",
      "Epoch 90/150: 100%|█████████████████████████| 855/855 [01:11<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 91/150: 100%|█████████████████████████| 820/820 [01:08<00:00, 11.94it/s, loss=0.015]\n",
      "Epoch 92/150: 100%|█████████████████████████| 863/863 [01:11<00:00, 12.00it/s, loss=0.016]\n",
      "Epoch 93/150: 100%|█████████████████████████| 807/807 [01:07<00:00, 11.95it/s, loss=0.015]\n",
      "Epoch 94/150: 100%|█████████████████████████| 873/873 [01:12<00:00, 12.01it/s, loss=0.016]\n",
      "Epoch 95/150: 100%|█████████████████████████| 830/830 [01:09<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 96/150: 100%|█████████████████████████| 817/817 [01:08<00:00, 11.97it/s, loss=0.018]\n",
      "Epoch 97/150: 100%|█████████████████████████| 833/833 [01:09<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 98/150: 100%|█████████████████████████| 833/833 [01:09<00:00, 11.95it/s, loss=0.015]\n",
      "Epoch 99/150: 100%|█████████████████████████| 860/860 [01:11<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 100/150: 100%|████████████████████████| 816/816 [01:08<00:00, 11.95it/s, loss=0.016]\n",
      "Epoch 101/150: 100%|████████████████████████| 837/837 [01:09<00:00, 11.96it/s, loss=0.015]\n",
      "Epoch 102/150: 100%|████████████████████████| 840/840 [01:10<00:00, 11.96it/s, loss=0.015]\n",
      "Epoch 103/150: 100%|████████████████████████| 838/838 [01:10<00:00, 11.96it/s, loss=0.015]\n",
      "Epoch 104/150: 100%|████████████████████████| 854/854 [01:11<00:00, 11.98it/s, loss=0.015]\n",
      "Epoch 105/150: 100%|████████████████████████| 851/851 [01:11<00:00, 11.98it/s, loss=0.015]\n",
      "Epoch 106/150: 100%|████████████████████████| 822/822 [01:08<00:00, 11.95it/s, loss=0.015]\n",
      "Epoch 107/150: 100%|████████████████████████| 871/871 [01:12<00:00, 12.00it/s, loss=0.016]\n",
      "Epoch 108/150: 100%|████████████████████████| 825/825 [01:08<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 109/150: 100%|████████████████████████| 823/823 [01:08<00:00, 11.96it/s, loss=0.015]\n",
      "Epoch 110/150: 100%|████████████████████████| 846/846 [01:10<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 111/150: 100%|████████████████████████| 824/824 [01:08<00:00, 11.99it/s, loss=0.017]\n",
      "Epoch 112/150: 100%|████████████████████████| 824/824 [01:08<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 113/150: 100%|████████████████████████| 828/828 [01:09<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 114/150: 100%|████████████████████████| 862/862 [01:11<00:00, 12.02it/s, loss=0.016]\n",
      "Epoch 115/150: 100%|████████████████████████| 819/819 [01:08<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 116/150: 100%|████████████████████████| 835/835 [01:09<00:00, 11.99it/s, loss=0.015]\n",
      "Epoch 117/150: 100%|████████████████████████| 844/844 [01:10<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 118/150: 100%|████████████████████████| 840/840 [01:10<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 119/150: 100%|████████████████████████| 876/876 [01:12<00:00, 12.02it/s, loss=0.016]\n",
      "Epoch 120/150: 100%|████████████████████████| 830/830 [01:09<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 121/150: 100%|████████████████████████| 821/821 [01:08<00:00, 12.00it/s, loss=0.015]\n",
      "Epoch 122/150: 100%|████████████████████████| 828/828 [01:09<00:00, 11.98it/s, loss=0.015]\n",
      "Epoch 123/150: 100%|████████████████████████| 839/839 [01:09<00:00, 12.00it/s, loss=0.017]\n",
      "Epoch 124/150: 100%|████████████████████████| 830/830 [01:09<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 125/150: 100%|████████████████████████| 845/845 [01:10<00:00, 12.01it/s, loss=0.015]\n",
      "Epoch 126/150: 100%|████████████████████████| 818/818 [01:08<00:00, 12.02it/s, loss=0.017]\n",
      "Epoch 127/150: 100%|████████████████████████| 840/840 [01:10<00:00, 11.96it/s, loss=0.017]\n",
      "Epoch 128/150: 100%|████████████████████████| 857/857 [01:11<00:00, 12.01it/s, loss=0.016]\n",
      "Epoch 129/150: 100%|████████████████████████| 808/808 [01:07<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 130/150: 100%|████████████████████████| 836/836 [01:09<00:00, 12.00it/s, loss=0.016]\n",
      "Epoch 131/150: 100%|████████████████████████| 817/817 [01:08<00:00, 11.95it/s, loss=0.016]\n",
      "Epoch 132/150: 100%|████████████████████████| 849/849 [01:10<00:00, 12.00it/s, loss=0.016]\n",
      "Epoch 133/150: 100%|████████████████████████| 838/838 [01:09<00:00, 12.00it/s, loss=0.017]\n",
      "Epoch 134/150: 100%|████████████████████████| 855/855 [01:11<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 135/150: 100%|████████████████████████| 827/827 [01:09<00:00, 11.97it/s, loss=0.016]\n",
      "Epoch 136/150: 100%|████████████████████████| 839/839 [01:10<00:00, 11.97it/s, loss=0.015]\n",
      "Epoch 137/150: 100%|████████████████████████| 849/849 [01:10<00:00, 12.01it/s, loss=0.017]\n",
      "Epoch 138/150: 100%|████████████████████████| 834/834 [01:09<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 139/150: 100%|████████████████████████| 829/829 [01:09<00:00, 11.98it/s, loss=0.016]\n",
      "Epoch 140/150: 100%|████████████████████████| 843/843 [01:10<00:00, 11.99it/s, loss=0.017]\n",
      "Epoch 141/150: 100%|████████████████████████| 833/833 [01:09<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 142/150: 100%|████████████████████████| 840/840 [01:10<00:00, 11.99it/s, loss=0.017]\n",
      "Epoch 143/150: 100%|████████████████████████| 849/849 [01:10<00:00, 12.01it/s, loss=0.016]\n",
      "Epoch 144/150: 100%|████████████████████████| 805/805 [01:07<00:00, 11.99it/s, loss=0.016]\n",
      "Epoch 145/150: 100%|████████████████████████| 835/835 [01:09<00:00, 12.00it/s, loss=0.016]\n",
      "Epoch 146/150: 100%|████████████████████████| 808/808 [01:07<00:00, 11.98it/s, loss=0.015]\n",
      "Epoch 147/150: 100%|████████████████████████| 807/807 [01:07<00:00, 12.00it/s, loss=0.017]\n",
      "Epoch 148/150: 100%|████████████████████████| 848/848 [01:10<00:00, 12.00it/s, loss=0.016]\n",
      "Epoch 149/150: 100%|████████████████████████| 853/853 [01:11<00:00, 11.95it/s, loss=0.017]\n",
      "Epoch 150/150: 100%|████████████████████████| 886/886 [01:13<00:00, 11.99it/s, loss=0.016]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "EPOCHS = 150\n",
    "RUN_PATH = \"runs/riichi-dsv02-mv01\"\n",
    "\n",
    "timestamp = datetime.datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "full_writer = SummaryWriter(f\"{RUN_PATH}/{timestamp}\")\n",
    "self_writer = SummaryWriter(f\"{RUN_PATH}/{timestamp}/self\")\n",
    "early_writer = SummaryWriter(f\"{RUN_PATH}/{timestamp}/early\")\n",
    "mid_writer = SummaryWriter(f\"{RUN_PATH}/{timestamp}/mid\")\n",
    "late_writer = SummaryWriter(f\"{RUN_PATH}/{timestamp}/late\")\n",
    "\n",
    "model = ModelV1()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "get_mae = torch.nn.L1Loss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_deque = deque(maxlen=len(full_dl))\n",
    "\n",
    "write_counter = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm.tqdm(\n",
    "        range(len(full_dl)), \n",
    "        desc=f\"Epoch {epoch+1}/{EPOCHS}\", \n",
    "        position=0, leave=True, ncols=90)\n",
    "    \n",
    "    model.train(True)\n",
    "    for i, (x, y) in enumerate(full_dl):\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        full_mae = get_mae(pred, y) * 25000\n",
    "        self_mae = get_mae(pred[:, 0], y[:, 0]) * 25000\n",
    "        tile_nums = torch.sum(x[:, 7:23], dim=(1, 2, 3))\n",
    "        early_game, mid_game, late_game = tile_nums <= 24, (24 < tile_nums) & (tile_nums <= 48), 48 <= tile_nums\n",
    "        early_mae = get_mae(pred[early_game], y[early_game]) * 25000\n",
    "        mid_mae = get_mae(pred[mid_game], y[mid_game]) * 25000\n",
    "        late_mae = get_mae(pred[late_game], y[late_game]) * 25000\n",
    "\n",
    "        loss_deque.append(loss.item())\n",
    "        full_writer.add_scalar(\"loss\", loss.item(), write_counter)\n",
    "        full_writer.add_scalar(\"score_mae\", full_mae, write_counter)\n",
    "        self_writer.add_scalar(\"score_mae\", self_mae, write_counter)\n",
    "        early_writer.add_scalar(\"score_mae\", early_mae, write_counter)\n",
    "        mid_writer.add_scalar(\"score_mae\", mid_mae, write_counter)\n",
    "        late_writer.add_scalar(\"score_mae\", late_mae, write_counter)\n",
    "        write_counter += 1\n",
    "        pbar.set_postfix({\"loss\": f\"{sum(loss_deque) / len(loss_deque):.3f}\"})\n",
    "        pbar.update()\n",
    "    full_ds.load_next()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fc27c-290a-4069-8bce-2f847e2abba3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d51957-ccf9-44c7-afd4-47fb70cc0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3204fb8f-a9a4-43ec-b569-1d7c4e183fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1118.0691,  -2460.5576,   2965.7051,    849.8802],\n",
       "        [    16.0355,   3333.3032,   1550.1235,  -4873.4150],\n",
       "        [ -3164.3574,   6006.6348,   -364.6023,  -2448.3506],\n",
       "        [ -5868.3174,   3699.3816,    590.9872,   1628.3444],\n",
       "        [ -2604.8049,  -1888.7352,   1894.8672,   2862.0574],\n",
       "        [   333.4638,    873.0424,  -1424.9915,    399.4430],\n",
       "        [  3947.2683,   -760.9034,   -234.7463,  -2356.8179],\n",
       "        [ -3137.8054,    892.3446,  -1313.4896,   3474.6860],\n",
       "        [  -739.7477,    338.8961,    307.7733,      0.3733],\n",
       "        [  4938.5493,    -55.2997,  -2705.8667,  -2094.5667],\n",
       "        [  3846.3994,   5107.6597,  -2973.0164,  -5757.3398],\n",
       "        [ -3107.0449,   4115.6987,  -1431.2545,    387.8928],\n",
       "        [  -193.6262,  -3247.8831,   -585.0303,   4402.7236],\n",
       "        [ -1435.0062,  -1394.9271,  -2990.1636,   5841.6812],\n",
       "        [    84.0741,  -1261.3650,    513.6969,    843.0516],\n",
       "        [   607.7133,   -226.2752,   -448.8474,    332.2622],\n",
       "        [ -2790.7727,   2008.6227,  -4073.6963,   5069.7896],\n",
       "        [  3607.5151,  -3510.6458,   2343.2307,  -2503.6304],\n",
       "        [ -1799.3689,  -2893.0950,   2237.2153,   2516.4570],\n",
       "        [ -2078.1562,   3192.7734,   4765.1099,  -5619.0811],\n",
       "        [ -2712.1794,    170.5323,    859.2482,   2266.2954],\n",
       "        [  -248.5570,  -1276.6573,   -723.4590,   2177.3494],\n",
       "        [ -2838.8052,   2041.3401,   3809.4277,  -2960.6121],\n",
       "        [ -1464.9708,   -883.6955,   1035.2109,   1400.6672],\n",
       "        [ -2647.8469,  -1626.8728,   2433.4368,   2158.5081],\n",
       "        [  6440.2490,  -5298.8696,   -263.5973,   -634.9312],\n",
       "        [  -752.5006,   3351.9438,  -3509.5312,   1012.6255],\n",
       "        [  2659.6724,    875.4473,  -4285.5142,    905.4508],\n",
       "        [ -3143.1750,  -1401.2050,   3961.9058,    862.4678],\n",
       "        [ -1691.0291,   -370.9588,   1335.1349,    725.2356],\n",
       "        [  4557.3032,  -4469.5010,  -2937.6230,   3128.7805],\n",
       "        [  2382.9004,  -4128.0518,   1617.2389,    489.1402],\n",
       "        [   -33.0236,    548.7618,  -1643.8557,   1512.3295],\n",
       "        [  4399.1270,   2791.2788,     -1.8043,  -6971.5156],\n",
       "        [ -1894.3634,  -3579.5991,   1311.5853,   4259.2710],\n",
       "        [  -875.2112,  -3343.1467,   2142.7791,   2190.9990],\n",
       "        [  3593.8545,  -1271.5189,  -1903.6252,   -201.8199],\n",
       "        [  2052.7805,  -3472.7595,  -1037.0133,   2524.2544],\n",
       "        [  1994.3857,    851.2244,  -3667.3162,    829.3367],\n",
       "        [  2486.3059,    -47.8219,  -1590.3030,   -659.3351],\n",
       "        [ -2327.2588,    766.0079,     29.1644,   1692.3716],\n",
       "        [  2214.1606,    153.4303,  -1250.3672,  -1113.8722],\n",
       "        [  2608.9849,  -2521.6121,    975.4240,  -1076.8674],\n",
       "        [    25.7003,    855.4908,    606.6855,  -1629.2256],\n",
       "        [ -4742.1826,   1567.9241,   2565.1113,    998.5430],\n",
       "        [ -5072.8867,   2404.7229,    158.7967,   3019.2686],\n",
       "        [ -2948.2476,  -2368.9956,   1790.3041,   3545.2104],\n",
       "        [ -3273.9622,   1582.0094,   3508.8677,  -1843.0592],\n",
       "        [ -1112.2375,  -4612.0771,   1950.2429,   3854.2651],\n",
       "        [  -548.7554,   1801.6904,    763.4337,  -1837.0634],\n",
       "        [ -2926.7063,   2993.0142,   2376.4573,  -2301.3574],\n",
       "        [ -3628.6018,  -3165.1030,   4818.3765,   2115.1184],\n",
       "        [  -368.4585,    525.5046,   -736.1819,   1166.7987],\n",
       "        [  -652.1053,   2214.9319,   3254.8550,  -4423.1401],\n",
       "        [   421.8177,  -1721.6660,  -1431.9504,   2913.5044],\n",
       "        [ -2741.8379,   -962.5396,  -2997.1257,   6861.1064],\n",
       "        [ -1276.9023,     61.2787,    890.4795,    432.0528],\n",
       "        [ -1140.3770,   5809.0645,  -1665.1985,  -2859.2192],\n",
       "        [ -2534.1670,   -866.0337,  -2702.7424,   6099.9453],\n",
       "        [ -1179.9242,  -3038.0715,    674.1655,   3563.4451],\n",
       "        [  3565.3955,  -3749.7222,   -930.2501,   1383.5587],\n",
       "        [  1402.2518,   4472.7324,  -1701.5770,  -4096.3887],\n",
       "        [  2298.4360,     11.1826,   -103.4617,  -2064.8010],\n",
       "        [  2099.0146,   4162.4697,  -2499.0073,  -3397.2842]], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "x, y = next(iter(full_dl))\n",
    "x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "\n",
    "model(x) * 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd5178c-8045-4eb1-934a-5218556ea3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0.0000,  -1000.0005,      0.0000,    999.9990],\n",
       "        [     0.0000,  16999.9980,      0.0000, -17000.0000],\n",
       "        [   999.9990,      0.0000,   -999.9990,      0.0000],\n",
       "        [ -1000.0005,   -999.9990,   3000.0000,  -1000.0005],\n",
       "        [  -400.0008,   -599.9998,   1399.9999,   -400.0008],\n",
       "        [  -999.9990,   6000.0000,   -999.9990,  -3000.0000],\n",
       "        [     0.0000,      0.0000,  -1800.0006,   2799.9998],\n",
       "        [     0.0000,      0.0000,  -1000.0020,   1000.0005],\n",
       "        [     0.0000,  -8700.0000,   8700.0000,      0.0000],\n",
       "        [   999.9990,  -3000.0000,      0.0000,      0.0000],\n",
       "        [ 13299.9990,   7699.9990,      0.0000, -20000.0000],\n",
       "        [     0.0000,      0.0000,  -8300.0000,   8300.0000],\n",
       "        [     0.0000,   7999.9985,  -8000.0015,      0.0000],\n",
       "        [ -1999.9996,  -1999.9996,  -1999.9996,   7000.0010],\n",
       "        [     0.0000,  16000.0010,      0.0000, -16000.0000],\n",
       "        [     0.0000,      0.0000,  -7700.0005,   7700.0005],\n",
       "        [     0.0000,      0.0000,  19000.0000, -19000.0000],\n",
       "        [     0.0000,   2299.9988,  -2300.0002,      0.0000],\n",
       "        [ -1500.0016,    499.9995,   1500.0000,  -1499.9985],\n",
       "        [  1500.0000,   1500.0000,  -1500.0016,  -1500.0000],\n",
       "        [ -2000.0011,  -3900.0005,  -1999.9996,   7900.0010],\n",
       "        [ -1499.9985,    499.9995,    499.9999,  -1500.0000],\n",
       "        [ -1999.9996,   6999.9995,  -3000.0000,  -1999.9996],\n",
       "        [ -1499.9985,    499.9995,    499.9999,  -1500.0000],\n",
       "        [     0.0000,  -3900.0005,      0.0000,   3900.0005],\n",
       "        [     0.0000,  -7699.9990,      0.0000,   7700.0005],\n",
       "        [ -3900.0005,      0.0000,   3900.0005,      0.0000],\n",
       "        [  -500.0010,   -499.9995,   2699.9980,   -699.9999],\n",
       "        [  5000.0010,  -1999.9996,  -2000.0011,   -999.9990],\n",
       "        [  5500.0005,  -1400.0013,  -2700.0010,  -1400.0006],\n",
       "        [ 12899.9990,      0.0000, -12900.0010,      0.0000],\n",
       "        [  3000.0000,   -999.9998,  -1000.0020,  -1000.0020],\n",
       "        [ -1100.0007,  -1099.9993,   4300.0000,  -2099.9998],\n",
       "        [     0.0000,  -8000.0000,   7999.9985,      0.0000],\n",
       "        [ -4000.0022,  -3999.9993,  -4000.0007,  11999.9990],\n",
       "        [  -499.9995,  -1000.0005,   3000.0000,   -499.9995],\n",
       "        [  7999.9985,  -8000.0000,      0.0000,      0.0000],\n",
       "        [ -3000.0000,    999.9990,      0.0000,    999.9990],\n",
       "        [     0.0000,    999.9990,  -1000.0005,      0.0000],\n",
       "        [ -4200.0010,  -9200.0010,  -5199.9995,  18599.9961],\n",
       "        [ -1000.0005,   -999.9990,   3000.0000,  -1000.0005],\n",
       "        [  -999.9990,   3000.0000,   -999.9990,  -1000.0005],\n",
       "        [ -3999.9993,  -4000.0007,  11999.9990,  -4000.0007],\n",
       "        [ -3899.9976,      0.0000,   3900.0005,      0.0000],\n",
       "        [     0.0000,  -1999.9996,      0.0000,   2000.0011],\n",
       "        [ -5200.0000,  -4200.0010,  -4199.9995,  15599.9990],\n",
       "        [ -1100.0007,   2300.0002,   -599.9998,   -599.9982],\n",
       "        [ -1399.9999,  -2699.9980,   5499.9990,  -1399.9999],\n",
       "        [ -1500.0000,  -1500.0016,   1500.0000,   1500.0016],\n",
       "        [ -3999.9993,   9000.0000,  -2000.0001,  -2000.0011],\n",
       "        [  -499.9995,   -300.0006,   1100.0007,   -300.0021],\n",
       "        [     0.0000,      0.0000,  -2000.0011,   1999.9980],\n",
       "        [  1500.0016,   1500.0000,  -1500.0016,  -1499.9985],\n",
       "        [ -2599.9993,   8800.0000,  -2599.9993,  -2600.0007],\n",
       "        [  6399.9980,  -5400.0005,      0.0000,      0.0000],\n",
       "        [     0.0000,  -3200.0005,      0.0000,   4199.9995],\n",
       "        [  5200.0015,      0.0000,  -5200.0000,      0.0000],\n",
       "        [ -3200.0005,   3200.0005,      0.0000,      0.0000],\n",
       "        [     0.0000,  -5500.0005,      0.0000,   5499.9990],\n",
       "        [ -6000.0000,  -5999.9995,  18000.0000,  -6000.0000],\n",
       "        [     0.0000,      0.0000,  -1999.9996,   1999.9996],\n",
       "        [ -5499.9990,      0.0000,   5499.9990,      0.0000],\n",
       "        [     0.0000,  12300.0010, -12300.0000,      0.0000],\n",
       "        [  2000.0011,   -499.9995,  -1000.0005,   -499.9995]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y * 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e67be1ad-e6f5-4e05-ae0b-6e020e72214d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9ee2c95-e47d-4a9a-8f29-130b2ea0c23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732f61d-4e08-4b7c-b45f-2cdc43bb883e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
